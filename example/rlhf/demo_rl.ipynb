{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# # Add the root folder to the module search path\n",
    "# # Get the current directory\n",
    "# current_directory = os.getcwd()\n",
    "\n",
    "# # Move two levels up (go to the parent directory of the parent directory)\n",
    "# # two_levels_up_directory = os.path.dirname(os.path.dirname(current_directory))\n",
    "# one_levels_up_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# print(one_levels_up_directory)\n",
    "\n",
    "# sys.path.append(two_levels_up_directory)\n",
    "sys.path.append(\"/home/ubuntu/git/pykoi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerate Config (Once per machine)\n",
    "reference: https://huggingface.co/docs/accelerate/basic_tutorials/notebook\n",
    "\n",
    "\n",
    "```\n",
    "(pykoi) $ accelerate config\n",
    "----------------------------------------------------------------------------------In which compute environment are you running?\n",
    "This machine                                                                      \n",
    "----------------------------------------------------------------------------------Which type of machine are you using?                                              \n",
    "multi-CPU                                                                         \n",
    "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1                                                                          \n",
    "Do you want to use Intel PyTorch Extension (IPEX) to speed up training on CPU? [yes/NO]:No                                                                          \n",
    "Do you wish to optimize your script with torch dynamo?[yes/NO]:No                 \n",
    "How many CPU(s) should be used for distributed training? [1]:8                    \n",
    "----------------------------------------------------------------------------------Do you wish to use FP16 or BF16 (mixed precision)?\n",
    "fp16                                                                              \n",
    "accelerate configuration saved at /home/ubuntu/.cache/huggingface/accelerate/default_config.yaml   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykoi\n",
    "from pykoi import RLHFConfig\n",
    "\n",
    "from accelerate import notebook_launcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from pykoi.chat.db.qa_database import QuestionAnswerDatabase\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, TaskType, get_peft_model\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import (Adafactor, AutoModelForCausalLM,\n",
    "                          AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          Trainer, TrainerCallback, TrainingArguments, logging,\n",
    "                          pipeline, set_seed)\n",
    "from transformers.utils import PushToHubMixin\n",
    "from trl import (AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer,\n",
    "                 SFTTrainer)\n",
    "from trl.core import LengthSampler\n",
    "from trl.trainer.utils import ConstantLengthDataset, PeftSavingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_CSV_HEADER_ID = 'ID'\n",
    "QA_CSV_HEADER_QUESTION = 'Question'\n",
    "QA_CSV_HEADER_ANSWER = 'Answer'\n",
    "QA_CSV_HEADER_VOTE_STATUS = 'Vote Status'\n",
    "QA_CSV_HEADER_TIMESTAMPS = 'Timestamp'\n",
    "QA_CSV_HEADER = (\n",
    "    QA_CSV_HEADER_ID,\n",
    "    QA_CSV_HEADER_QUESTION,\n",
    "    QA_CSV_HEADER_ANSWER,\n",
    "    QA_CSV_HEADER_VOTE_STATUS,\n",
    "    QA_CSV_HEADER_TIMESTAMPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL(Trainer):\n",
    "    def __init__(self, rlhf_config: RLHFConfig):\n",
    "        self._rlhf_config = rlhf_config\n",
    "        self.accelerator = Accelerator()\n",
    "        self.num_proc = self._rlhf_config.num_workers if not self._rlhf_config.streaming else None\n",
    "        set_seed(rlhf_config.seed) ## TODO: how to set seed properly in __init__?\n",
    "\n",
    "        self.ppo_config=PPOConfig(\n",
    "            steps=self._rlhf_config.total_ppo_epochs,\n",
    "            model_name=self._rlhf_config.base_model_path,\n",
    "            learning_rate=self._rlhf_config.learning_rate,\n",
    "            batch_size=self._rlhf_config.ppo_batch_size,\n",
    "            mini_batch_size=self._rlhf_config.mini_batch_size,\n",
    "            gradient_accumulation_steps=self._rlhf_config.gradient_accumulation_steps,\n",
    "            optimize_cuda_cache=True,\n",
    "            early_stopping=self._rlhf_config.early_stopping,\n",
    "            target_kl=self._rlhf_config.target_kl,\n",
    "            ppo_epochs=self._rlhf_config.ppo_epochs,\n",
    "            seed=self._rlhf_config.seed,\n",
    "            init_kl_coef=self._rlhf_config.init_kl_coef,\n",
    "            adap_kl_ctrl=self._rlhf_config.adap_kl_ctrl,\n",
    "            # accelerator_kwargs=self._rlhf_config.accelerator_kwargs,\n",
    "            )\n",
    "        \n",
    "        ## Load the base model and tokenizer and define the PPO Trainer for RL\n",
    "        self.base_tokenizer = self.create_tokenizer(rlhf_config.base_model_path)\n",
    "        self.base_dataset=self.create_dataset(self.base_tokenizer)\n",
    "        self.base_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            rlhf_config.base_model_path,\n",
    "            load_in_8bit=rlhf_config.load_in_8bit,\n",
    "            # is_loaded_in_8bit = True, # TODO TypeError: LlamaForCausalLM.__init__() got an unexpected keyword argument 'is_loaded_in_8bit'\n",
    "            # torch_dtype=torch.float16, \n",
    "            device_map={\"\": Accelerator().local_process_index},\n",
    "            peft_config=rlhf_config.lora_config_rl, \n",
    "        )\n",
    "        self.ppo_trainer = PPOTrainer(\n",
    "            config=self.ppo_config,\n",
    "            model=self.base_model,\n",
    "            ref_model=None,\n",
    "            tokenizer=self.base_tokenizer,\n",
    "            dataset=self.base_dataset,\n",
    "            data_collator=self.data_collator,\n",
    "            # optimizer=optimizer,\n",
    "            # peft_config=lora_config, ## PPOTrainer doesn't support parameter peft_config\n",
    "        )\n",
    "        self.base_kwargs = {\n",
    "            # \"min_length\": -1,\n",
    "            \"top_k\": rlhf_config.top_k,\n",
    "            \"top_p\": rlhf_config.top_p,\n",
    "            \"do_sample\": rlhf_config.do_sample,\n",
    "            \"pad_token_id\": self.base_tokenizer.pad_token_id,\n",
    "            \"eos_token_id\": rlhf_config.eos_token_id,\n",
    "            \"max_length\": rlhf_config.output_max_length\n",
    "        }\n",
    "\n",
    "        ## Load the reward model and tokenizer and define the reward pipeline\n",
    "        self.reward_tokenizer = self.create_tokenizer(rlhf_config.reward_model_path)\n",
    "        self.reward_dataset=self.create_dataset(self.reward_tokenizer)\n",
    "        self.reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            rlhf_config.reward_model_path, \n",
    "            num_labels=1,\n",
    "            # torch_dtype=torch.bfloat16,\n",
    "            load_in_8bit=True,\n",
    "            device_map={\"\": Accelerator().local_process_index}\n",
    "        )\n",
    "        self.reward_kwargs = {\n",
    "            \"return_all_scores\": True,\n",
    "            \"function_to_apply\": \"none\",\n",
    "            \"batch_size\": self._rlhf_config.ppo_batch_size,\n",
    "            \"truncation\": True,\n",
    "            \"max_length\": self._rlhf_config.output_max_length\n",
    "        }\n",
    "        self.reward_pipe = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=self.reward_model,\n",
    "            # device_map={\"\": Accelerator().local_process_index},\n",
    "            # model_kwargs={\"load_in_8bit\": True},\n",
    "            model_kwargs=self.reward_kwargs,\n",
    "            tokenizer=self.reward_tokenizer,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "\n",
    "        \n",
    "    def create_tokenizer(self, model_name):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        return tokenizer\n",
    "\n",
    "\n",
    "    def data_collator(self, data):\n",
    "        return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "\n",
    "    def create_dataset(self, tokenizer):\n",
    "        \"\"\"\n",
    "        Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "        customize this function to train the model on its own dataset.\n",
    "        \"\"\"\n",
    "        args = self._rlhf_config\n",
    "        if args.dataset_type == \"local_db\":\n",
    "            qa_database = QuestionAnswerDatabase()\n",
    "            my_data_pd = qa_database.retrieve_all_question_answers_as_pandas()\n",
    "            my_data_pd = my_data_pd[my_data_pd[QA_CSV_HEADER_VOTE_STATUS]==\"up\"]\n",
    "            my_data_pd = my_data_pd[[QA_CSV_HEADER_ID,\n",
    "                                     QA_CSV_HEADER_QUESTION,\n",
    "                                     QA_CSV_HEADER_ANSWER]]\n",
    "            print(\"My local database has {} samples\".format(my_data_pd.shape[0]))\n",
    "            dataset = Dataset.from_dict(my_data_pd)\n",
    "        elif args.dataset_type == \"local_csv\":\n",
    "            dataset = load_dataset('csv', data_files=args.dataset_name)\n",
    "            dataset = dataset[args.split] # Convert DatasetDict to Dataset\n",
    "        elif args.dataset_type == \"huggingface\":\n",
    "            dataset = load_dataset(\n",
    "                args.dataset_name,\n",
    "                data_dir=args.dataset_subset_sft,\n",
    "                split=args.split,\n",
    "                use_auth_token=True,\n",
    "                num_proc=self.num_proc,\n",
    "                streaming=args.streaming,\n",
    "            )\n",
    "            dataset = dataset[args.split] # Convert DatasetDict to Dataset\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No (supported) data files or dataset script found {args.dataset_type}\")\n",
    "        \n",
    "        # dataset = dataset.train_test_split(test_size=args.train_test_split_ratio, \n",
    "        #                                    seed=args.seed)\n",
    "        # print(f\"Size of the train set: {len(dataset['train'])}. \\\n",
    "        #       Size of the validation set: {len(dataset['test'])}\")\n",
    "        \n",
    "        # dataset = dataset.select(range(self._rlhf_config.dataset_subset_rl_train))\n",
    "\n",
    "        def preprocess_function(examples):\n",
    "            queries = [\"Question: \" + q + \"\\n\\nAnswer: \" for q in examples[QA_CSV_HEADER_QUESTION]]\n",
    "            input_ids = [tokenizer(q, truncation=True)[\"input_ids\"] for q in queries]\n",
    "            return {\"query\": queries, \"input_ids\": input_ids}\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=24, ## TODO self.num_proc,\n",
    "            remove_columns=dataset.column_names,\n",
    "        )\n",
    "        dataset = dataset.filter(lambda x: len(x[\"input_ids\"]) < self._rlhf_config.max_seq_length, \n",
    "                       batched=False)\n",
    "        dataset.set_format(type=\"torch\") ## TODO\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def _train(self):\n",
    "        for epoch, batch in tqdm(enumerate(self.ppo_trainer.dataloader)):\n",
    "            if epoch >= self._rlhf_config.total_ppo_epochs:\n",
    "                break\n",
    "            ## embed the questions and responses to tensors\n",
    "            question_tensors = batch[\"input_ids\"]\n",
    "            response_tensors = self.ppo_trainer.generate(\n",
    "                question_tensors,\n",
    "                return_prompt=False,\n",
    "                length_sampler=LengthSampler(self._rlhf_config.output_min_length, \n",
    "                                             self._rlhf_config.output_max_length),\n",
    "                **self.base_kwargs,\n",
    "            )\n",
    "            batch[QA_CSV_HEADER_ANSWER] = self.base_tokenizer.batch_decode(response_tensors, \n",
    "                                                                 skip_special_tokens=True)\n",
    "            # compute rewards and run PPO\n",
    "            texts = [q + r for q, r in zip(batch[\"query\"], batch[QA_CSV_HEADER_ANSWER])]\n",
    "            pipe_outputs = self.reward_pipe(texts, **self.reward_kwargs)\n",
    "            rewards = [torch.tensor(output[0][\"score\"] - self._rlhf_config.reward_baseline) \\\n",
    "                       for output in pipe_outputs]\n",
    "            stats = self.ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "            self.ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "            ## save weights\n",
    "            if self._rlhf_config.save_freq and epoch and \\\n",
    "                epoch % self._rlhf_config.save_freq == 0:\n",
    "                self.ppo_trainer.save_pretrained(\n",
    "                    os.path.join(self._rlhf_config.output_dir, f\"rlhf_rl_step_{epoch}\"))\n",
    "                \n",
    "    def train(self, num_processes=1):\n",
    "        notebook_launcher(self._train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My local database has 118 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/118 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]\n",
      "/opt/conda/envs/pykoi/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My local database has 118 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/118 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "                                                   \r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "config = pykoi.RLHFConfig(base_model_path=\"elinas/llama-7b-hf-transformers-4.29\", # \"elinas/llama-7b-hf-transformers-4.29\", \n",
    "                          dataset_type=\"local_db\",\n",
    "                          reward_model_path=\"goldmermaid/rlhf_reward_model\",\n",
    "                          save_freq=100,\n",
    "                          ppo_batch_size=32,\n",
    "                          ppo_epochs=1,\n",
    "\n",
    "                          )\n",
    "rlhf_step3_rl = RL(config)\n",
    "rlhf_step3_rl.train(\"./models/rlhf_step3_rl\", num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
