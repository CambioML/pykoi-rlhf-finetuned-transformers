{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# # Add the root folder to the module search path\n",
    "# # Get the current directory\n",
    "# current_directory = os.getcwd()\n",
    "\n",
    "# # Move two levels up (go to the parent directory of the parent directory)\n",
    "# two_levels_up_directory = os.path.dirname(os.path.dirname(current_directory))\n",
    "\n",
    "# print(two_levels_up_directory)\n",
    "\n",
    "# sys.path.append(two_levels_up_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyngrok\n",
    "# !pip install mlflow\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/koi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pykoi.chat import QuestionAnswerDatabase\n",
    "from pykoi.rlhf import RLHFConfig\n",
    "from pykoi.rlhf import SupervisedFinetuning\n",
    "import mlflow\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define my DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_database = QuestionAnswerDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert my data to the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is InstructGPT?</td>\n",
       "      <td>InstructGPT is a language model developed by O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why does InstructGPT work?</td>\n",
       "      <td>InstructGPT works due to a two-step training p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some commonly used evaluation metrics...</td>\n",
       "      <td>One main evaluation metric for InstructGPT is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How is InstructGPT used?</td>\n",
       "      <td>InstructGPT can be used in any application tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are some common applications of InstructGPT?</td>\n",
       "      <td>Common applications of InstructGPT can be in e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does InstructGPT handle ambiguous prompts?</td>\n",
       "      <td>For ambiguous prompts, InstructGPT aims to ask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Can InstructGPT generate incorrect or nonsensi...</td>\n",
       "      <td>Yes, InstructGPT can sometimes produce plausib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How does InstructGPT manage harmful and biased...</td>\n",
       "      <td>InstructGPT has a moderation system in place t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the role of human evaluators in the tr...</td>\n",
       "      <td>Human evaluators play a crucial role in the tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the limitations of InstructGPT?</td>\n",
       "      <td>There are several limitations to InstructGPT. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is reinforcement learning with human feed...</td>\n",
       "      <td>Reinforcement learning with human feedback is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How is human feedback integrated into the rein...</td>\n",
       "      <td>Human feedback can be integrated into reinforc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What are the advantages of reinforcement learn...</td>\n",
       "      <td>Reinforcement learning with human feedback can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What are some challenges of reinforcement lear...</td>\n",
       "      <td>One of the main challenges is the potential fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Can you give an example of an application of r...</td>\n",
       "      <td>One potential application of reinforcement lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is 'step 1 supervised finetuning' in rein...</td>\n",
       "      <td>'Step 1 supervised finetuning' in reinforcemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Why does step 1 supervised finetuning work in ...</td>\n",
       "      <td>Step 1 supervised finetuning works in reinforc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What are some commonly used evaluation metrics...</td>\n",
       "      <td>Common evaluation metrics in supervised finetu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How is step 1 supervised finetuning used in re...</td>\n",
       "      <td>In step 1 supervised finetuning, a model is fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What are some common applications of step 1 su...</td>\n",
       "      <td>Step 1 supervised finetuning can be used in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What is the role of human demonstrations in st...</td>\n",
       "      <td>Human demonstrations play a crucial role in st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>How is the training data collected for step 1 ...</td>\n",
       "      <td>The training data for step 1 supervised finetu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What are the limitations of step 1 supervised ...</td>\n",
       "      <td>While supervised finetuning can provide a stro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>How does step 1 supervised finetuning fit into...</td>\n",
       "      <td>Step 1 supervised finetuning is typically the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What are the prerequisites for using step 1 su...</td>\n",
       "      <td>To use step 1 supervised finetuning in reinfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What is 'reward modeling' in reinforcement lea...</td>\n",
       "      <td>Reward modeling is the second step in reinforc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Why does reward modeling work in reinforcement...</td>\n",
       "      <td>Reward modeling works because it bridges the g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>What are some commonly used evaluation metrics...</td>\n",
       "      <td>In reward modeling, one common evaluation metr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>How is reward modeling used in reinforcement l...</td>\n",
       "      <td>In reward modeling, a model is trained on a da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What are some common applications of reward mo...</td>\n",
       "      <td>Reward modeling can be applied in any scenario...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>What is the role of human feedback in reward m...</td>\n",
       "      <td>Human feedback plays a critical role in reward...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>How is the training data collected for reward ...</td>\n",
       "      <td>The training data for reward modeling is typic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>What are the limitations of reward modeling?</td>\n",
       "      <td>One limitation of reward modeling is that it r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>How does reward modeling fit into the broader ...</td>\n",
       "      <td>Reward modeling is typically the second step i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>What are the prerequisites for using reward mo...</td>\n",
       "      <td>To use reward modeling in reinforcement learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>What is training a reinforcement learning (RL)...</td>\n",
       "      <td>Training an RL model in this context refers to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Why does training an RL model work in reinforc...</td>\n",
       "      <td>Training an RL model works because it allows t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>What are some commonly used evaluation metrics...</td>\n",
       "      <td>The most common evaluation metric in training ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>How is training an RL model used in reinforcem...</td>\n",
       "      <td>In training an RL model, the model interacts w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>What are some common applications of training ...</td>\n",
       "      <td>Training an RL model can be used in a wide ran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>How can loss explosion be avoided during the t...</td>\n",
       "      <td>Loss explosion can be avoided through a variet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>What are common failures in training an RL model?</td>\n",
       "      <td>Common failures in training an RL model includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>What are the roles of exploration and exploita...</td>\n",
       "      <td>Exploration and exploitation play key roles in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>How does training an RL model fit into the bro...</td>\n",
       "      <td>Training an RL model is typically the third st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>What are the prerequisites for training an RL ...</td>\n",
       "      <td>To train an RL model in reinforcement learning...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Question  \\\n",
       "0                                What is InstructGPT?   \n",
       "1                          Why does InstructGPT work?   \n",
       "2   What are some commonly used evaluation metrics...   \n",
       "3                            How is InstructGPT used?   \n",
       "4   What are some common applications of InstructGPT?   \n",
       "5      How does InstructGPT handle ambiguous prompts?   \n",
       "6   Can InstructGPT generate incorrect or nonsensi...   \n",
       "7   How does InstructGPT manage harmful and biased...   \n",
       "8   What is the role of human evaluators in the tr...   \n",
       "9            What are the limitations of InstructGPT?   \n",
       "10  What is reinforcement learning with human feed...   \n",
       "11  How is human feedback integrated into the rein...   \n",
       "12  What are the advantages of reinforcement learn...   \n",
       "13  What are some challenges of reinforcement lear...   \n",
       "14  Can you give an example of an application of r...   \n",
       "15  What is 'step 1 supervised finetuning' in rein...   \n",
       "16  Why does step 1 supervised finetuning work in ...   \n",
       "17  What are some commonly used evaluation metrics...   \n",
       "18  How is step 1 supervised finetuning used in re...   \n",
       "19  What are some common applications of step 1 su...   \n",
       "20  What is the role of human demonstrations in st...   \n",
       "21  How is the training data collected for step 1 ...   \n",
       "22  What are the limitations of step 1 supervised ...   \n",
       "23  How does step 1 supervised finetuning fit into...   \n",
       "24  What are the prerequisites for using step 1 su...   \n",
       "25  What is 'reward modeling' in reinforcement lea...   \n",
       "26  Why does reward modeling work in reinforcement...   \n",
       "27  What are some commonly used evaluation metrics...   \n",
       "28  How is reward modeling used in reinforcement l...   \n",
       "29  What are some common applications of reward mo...   \n",
       "30  What is the role of human feedback in reward m...   \n",
       "31  How is the training data collected for reward ...   \n",
       "32       What are the limitations of reward modeling?   \n",
       "33  How does reward modeling fit into the broader ...   \n",
       "34  What are the prerequisites for using reward mo...   \n",
       "35  What is training a reinforcement learning (RL)...   \n",
       "36  Why does training an RL model work in reinforc...   \n",
       "37  What are some commonly used evaluation metrics...   \n",
       "38  How is training an RL model used in reinforcem...   \n",
       "39  What are some common applications of training ...   \n",
       "40  How can loss explosion be avoided during the t...   \n",
       "41  What are common failures in training an RL model?   \n",
       "42  What are the roles of exploration and exploita...   \n",
       "43  How does training an RL model fit into the bro...   \n",
       "44  What are the prerequisites for training an RL ...   \n",
       "\n",
       "                                               Answer  \n",
       "0   InstructGPT is a language model developed by O...  \n",
       "1   InstructGPT works due to a two-step training p...  \n",
       "2   One main evaluation metric for InstructGPT is ...  \n",
       "3   InstructGPT can be used in any application tha...  \n",
       "4   Common applications of InstructGPT can be in e...  \n",
       "5   For ambiguous prompts, InstructGPT aims to ask...  \n",
       "6   Yes, InstructGPT can sometimes produce plausib...  \n",
       "7   InstructGPT has a moderation system in place t...  \n",
       "8   Human evaluators play a crucial role in the tr...  \n",
       "9   There are several limitations to InstructGPT. ...  \n",
       "10  Reinforcement learning with human feedback is ...  \n",
       "11  Human feedback can be integrated into reinforc...  \n",
       "12  Reinforcement learning with human feedback can...  \n",
       "13  One of the main challenges is the potential fo...  \n",
       "14  One potential application of reinforcement lea...  \n",
       "15  'Step 1 supervised finetuning' in reinforcemen...  \n",
       "16  Step 1 supervised finetuning works in reinforc...  \n",
       "17  Common evaluation metrics in supervised finetu...  \n",
       "18  In step 1 supervised finetuning, a model is fi...  \n",
       "19  Step 1 supervised finetuning can be used in a ...  \n",
       "20  Human demonstrations play a crucial role in st...  \n",
       "21  The training data for step 1 supervised finetu...  \n",
       "22  While supervised finetuning can provide a stro...  \n",
       "23  Step 1 supervised finetuning is typically the ...  \n",
       "24  To use step 1 supervised finetuning in reinfor...  \n",
       "25  Reward modeling is the second step in reinforc...  \n",
       "26  Reward modeling works because it bridges the g...  \n",
       "27  In reward modeling, one common evaluation metr...  \n",
       "28  In reward modeling, a model is trained on a da...  \n",
       "29  Reward modeling can be applied in any scenario...  \n",
       "30  Human feedback plays a critical role in reward...  \n",
       "31  The training data for reward modeling is typic...  \n",
       "32  One limitation of reward modeling is that it r...  \n",
       "33  Reward modeling is typically the second step i...  \n",
       "34  To use reward modeling in reinforcement learni...  \n",
       "35  Training an RL model in this context refers to...  \n",
       "36  Training an RL model works because it allows t...  \n",
       "37  The most common evaluation metric in training ...  \n",
       "38  In training an RL model, the model interacts w...  \n",
       "39  Training an RL model can be used in a wide ran...  \n",
       "40  Loss explosion can be avoided through a variet...  \n",
       "41  Common failures in training an RL model includ...  \n",
       "42  Exploration and exploitation play key roles in...  \n",
       "43  Training an RL model is typically the third st...  \n",
       "44  To train an RL model in reinforcement learning...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_root = \"input/\"\n",
    "input_file = \"rlhf_qa_dataset.json\"\n",
    "my_stackoverflow_dataset = pd.read_json(input_root + input_file, orient=\"records\")\n",
    "my_stackoverflow_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_CSV_HEADER_ID = 'ID'\n",
    "QA_CSV_HEADER_QUESTION = 'Question'\n",
    "QA_CSV_HEADER_ANSWER = 'Answer'\n",
    "QA_CSV_HEADER_VOTE_STATUS = 'Vote Status'\n",
    "QA_CSV_HEADER_TIMESTAMPS = 'Timestamp'\n",
    "QA_CSV_HEADER = (\n",
    "    QA_CSV_HEADER_ID,\n",
    "    QA_CSV_HEADER_QUESTION,\n",
    "    QA_CSV_HEADER_ANSWER,\n",
    "    QA_CSV_HEADER_VOTE_STATUS,\n",
    "    QA_CSV_HEADER_TIMESTAMPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in my_stackoverflow_dataset.iloc[3:100].to_dict('records'):\n",
    "    qa_id = qa_database.insert_question_answer(question=row[QA_CSV_HEADER_QUESTION],\n",
    "                                       answer=row[QA_CSV_HEADER_ANSWER])\n",
    "    qa_database.update_vote_status(id=qa_id, vote_status=\"up\") #row[QA_CSV_HEADER_VOTE_STATUS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pykoi.chat.db.qa_database.QuestionAnswerDatabase at 0x7f7211a2a470>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RLHF using the data from database\n",
    "\n",
    "Let's take a look of the QA data and process it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Vote Status</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>How is InstructGPT used?</td>\n",
       "      <td>InstructGPT can be used in any application tha...</td>\n",
       "      <td>up</td>\n",
       "      <td>2023-10-04 22:19:07.475708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What are some common applications of InstructGPT?</td>\n",
       "      <td>Common applications of InstructGPT can be in e...</td>\n",
       "      <td>up</td>\n",
       "      <td>2023-10-04 22:19:07.486704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How does InstructGPT handle ambiguous prompts?</td>\n",
       "      <td>For ambiguous prompts, InstructGPT aims to ask...</td>\n",
       "      <td>up</td>\n",
       "      <td>2023-10-04 22:19:07.496303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Can InstructGPT generate incorrect or nonsensi...</td>\n",
       "      <td>Yes, InstructGPT can sometimes produce plausib...</td>\n",
       "      <td>up</td>\n",
       "      <td>2023-10-04 22:19:07.503408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>How does InstructGPT manage harmful and biased...</td>\n",
       "      <td>InstructGPT has a moderation system in place t...</td>\n",
       "      <td>up</td>\n",
       "      <td>2023-10-04 22:19:07.511437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                           Question  \\\n",
       "0   1                           How is InstructGPT used?   \n",
       "1   2  What are some common applications of InstructGPT?   \n",
       "2   3     How does InstructGPT handle ambiguous prompts?   \n",
       "3   4  Can InstructGPT generate incorrect or nonsensi...   \n",
       "4   5  How does InstructGPT manage harmful and biased...   \n",
       "\n",
       "                                              Answer Vote Status  \\\n",
       "0  InstructGPT can be used in any application tha...          up   \n",
       "1  Common applications of InstructGPT can be in e...          up   \n",
       "2  For ambiguous prompts, InstructGPT aims to ask...          up   \n",
       "3  Yes, InstructGPT can sometimes produce plausib...          up   \n",
       "4  InstructGPT has a moderation system in place t...          up   \n",
       "\n",
       "                    Timestamp  \n",
       "0  2023-10-04 22:19:07.475708  \n",
       "1  2023-10-04 22:19:07.486704  \n",
       "2  2023-10-04 22:19:07.496303  \n",
       "3  2023-10-04 22:19:07.503408  \n",
       "4  2023-10-04 22:19:07.511437  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data_pd = qa_database.retrieve_all_question_answers_as_pandas()\n",
    "my_data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data_pd = my_data_pd[my_data_pd[QA_CSV_HEADER_VOTE_STATUS]==\"up\"]\n",
    "my_data_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My local database has 126 samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'Question', 'Answer'],\n",
       "    num_rows: 126\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "my_data_pd = my_data_pd[[QA_CSV_HEADER_ID,\n",
    "                        QA_CSV_HEADER_QUESTION,\n",
    "                        QA_CSV_HEADER_ANSWER]]\n",
    "print(\"My local database has {} samples\".format(my_data_pd.shape[0]))\n",
    "dataset = Dataset.from_dict(my_data_pd)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up mlflow experiment name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/04 22:22:12 INFO mlflow.tracking.fluent: Experiment with name 'rlhf_step1_sft/2023-10-04 22:22:12.103672' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/ubuntu/pykoi/example/rlhf/mlflow/mlruns/960009078910101657', creation_time=1696458132114, experiment_id='960009078910101657', last_update_time=1696458132114, lifecycle_stage='active', name='rlhf_step1_sft/2023-10-04 22:22:12.103672', tags={}>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlflow.set_tracking_uri(\"http://x.x.x.x:5000\")\n",
    "experiment = \"rlhf_step1_sft\"\n",
    "current_time = str(datetime.datetime.now())\n",
    "mlflow_experiment_name = '/'.join([experiment, current_time])\n",
    "\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    print(\"No mlflow run in progress\")\n",
    "\n",
    "mlflow.set_experiment(mlflow_experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set pykoi parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"elinas/llama-7b-hf-transformers-4.29\"\n",
    "dataset_type = \"local_db\"\n",
    "log_freq = 1\n",
    "max_steps = 5\n",
    "peft_model_path = \"./models/rlhf_step1_sft\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually log pykoi parameters into mlflow. Torch level parameters are automatically logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/rlhf_step1_sft'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.log_param(\"pykoi_base_model_path\", base_model_path)\n",
    "mlflow.log_param(\"pykoi_dataset_type\", dataset_type)\n",
    "mlflow.log_param(\"pykoi_log_freq\", log_freq)\n",
    "mlflow.log_param(\"pykoi_max_steps\", max_steps)\n",
    "mlflow.log_param(\"pykoi_peft_model_path\", peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training metrics are automatically logged into mlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My local database has 126 up vote samples for SFT\n",
      "Size of the train set: 113.               Size of the validation set: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/koi/lib/python3.10/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:49<00:00, 54.76s/it]\n",
      "/home/ubuntu/miniconda3/envs/koi/lib/python3.10/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "/home/ubuntu/miniconda3/envs/koi/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/koi/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/koi/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/koi/lib/python3.10/site-packages/trl/trainer/utils.py:268: UserWarning: The dataset reached end and the iterator is reset to the start.\n",
      "  warnings.warn(\"The dataset reached end and the iterator is reset to the start.\")\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/ubuntu/miniconda3/envs/koi/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 01:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run supervised finetuning\n",
    "config = RLHFConfig(\n",
    "    base_model_path=base_model_path, \n",
    "    dataset_type=dataset_type,\n",
    "    max_steps=max_steps,\n",
    "    log_freq=log_freq\n",
    "    )\n",
    "rlhf_step1_sft = SupervisedFinetuning(config)\n",
    "rlhf_step1_sft.train_and_save(peft_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained peft model and input into mlflow artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_artifacts(peft_model_path)\n",
    "mlflow.log_artifacts(input_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the terminal, run\n",
    "```\n",
    "mlflow ui\n",
    "```\n",
    "and go to http://127.0.0.1:5000 in the browser to view the experiment in the UI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
