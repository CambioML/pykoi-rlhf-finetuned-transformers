{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: LLMs Comparison between Open-source LLMs\n",
    "\n",
    "`pykoi` provides simple API to compare between LLMs, including your own finetuned LLM, a pretrained LLM from huggingface, or OpenAI/Anthropic/Bedrock APIs. This demo shows how to create and launch an LLM comparison app between Open-source LLMs from huggingface. Let's get started!\n",
    "\n",
    "### Prerequisites\n",
    "To run this jupyter notebook, you need a `pykoi` environment with the `huggingface` option. You can follow [the installation guide](https://github.com/CambioML/pykoi/tree/install#option-2-rag-gpu) to set up the environment. \n",
    "\n",
    "You may also need `pip install ipykernel` to run the kernel environment.\n",
    "\n",
    "\n",
    "### (Optional) Developer setup\n",
    "If you are a normal user of `pykoi`, you can skip this step. However, if you modify the pykoi code and want to test your changes, you can uncomment the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# import sys\n",
    "\n",
    "# sys.path.append(\".\")\n",
    "# sys.path.append(\"..\")\n",
    "# sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pykoi import Application\n",
    "from pykoi.chat import ModelFactory\n",
    "from pykoi.chat import QuestionAnswerDatabase\n",
    "from pykoi.component import Chatbot, Dashboard, Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLMs\n",
    "#### Creating a Huggingface model (requires at least EC2 `g4dn.xlarge` or GPU with at least 16G memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pykoi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/pykoi/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HuggingfaceModel] loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pykoi/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/envs/pykoi/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HuggingfaceModel] loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "## requires a GPU with at least 16GB memory (e.g. g4dn.xlarge)\n",
    "huggingface_model_1 = ModelFactory.create_model(\n",
    "    model_source=\"huggingface\",\n",
    "    pretrained_model_name_or_path=\"tiiuae/falcon-rw-1b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HuggingfaceModel] loading model...\n",
      "[HuggingfaceModel] loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "## requires a GPU with at least 16GB memory (e.g. g4dn.2xlarge)\n",
    "huggingface_model_2 = ModelFactory.create_model(\n",
    "    model_source=\"huggingface\",\n",
    "    pretrained_model_name_or_path=\"databricks/dolly-v2-3b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HuggingfaceModel] loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:52<00:00, 56.27s/it]\n",
      "Downloading generation_config.json: 100%|██████████| 117/117 [00:00<00:00, 1.03MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HuggingfaceModel] loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 287/287 [00:00<00:00, 2.68MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 2.73M/2.73M [00:00<00:00, 70.6MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 281/281 [00:00<00:00, 2.27MB/s]\n"
     ]
    }
   ],
   "source": [
    "## requires a GPU with at least 24GB memory (e.g. g5.2xlarge)\n",
    "huggingface_model_3 = ModelFactory.create_model(\n",
    "    model_source=\"huggingface\",\n",
    "    pretrained_model_name_or_path=\"tiiuae/falcon-7b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a chatbot comparator\n",
    "\n",
    "#### Add `nest_asyncio` \n",
    "Add `nest_asyncio` to avoid error. Since we're running another interface inside a Jupyter notebook where an asyncio event loop is already running, we'll encounter the error. (since The uvicorn.run() function uses asyncio.run(), which isn't compatible with a running event loop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in a list of models to compare\n",
    "chatbot_comparator = Compare(models=[huggingface_model_1, huggingface_model_2, huggingface_model_3])\n",
    "# chatbot_comparator.add(openai_model_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add ngrok auth (TODO: change to bash file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ngrok config add-authtoken xxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [11390]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "app = Application(debug=False, share=False)\n",
    "app.add_component(chatbot_comparator)\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can launch the ranking chatbot. Click this above link and you can similar interface:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../image/drag_and_rank_crop_2x.gif\" width=\"75%\" height=\"75%\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "You can also check the dashboard for your ranking of the model answers:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../image/comparisonDemoSmall_2x.gif\" width=\"75%\" height=\"75%\" />\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
