{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch a Chatbot UI (with Database) from Open-source LLMs\n",
    "\n",
    "`pykoi` provides simple UI to launch a chatbot UI based on your LLMs, including your own finetuned LLM, a pretrained LLM from huggingface, or OpenAI/Anthropic/Bedrock APIs. This demo shows how to create and launch an LLM chatbot UI and database (with Database) from Open-source LLMs from Huggingface. Let's get started!\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "To run this jupyter notebook, you need a `pykoi` environment with the `huggingface` option. You can follow [the installation guide](https://github.com/CambioML/pykoi/tree/install#option-2-rag-gpu) to set up the environment. \n",
    "\n",
    "You may also need `pip install ipykernel` to run the kernel environment.\n",
    "\n",
    "\n",
    "### (Optional) Developer setup\n",
    "If you are a normal user of `pykoi`, you can skip this step. However, if you modify the pykoi code and want to test your changes, you can uncomment the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# import sys\n",
    "\n",
    "# sys.path.append(\".\")\n",
    "# sys.path.append(\"..\")\n",
    "# sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install ipykernel\n",
    "from pykoi import Application\n",
    "from pykoi.chat import ModelFactory\n",
    "from pykoi.chat import QuestionAnswerDatabase\n",
    "from pykoi.component import Chatbot, Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a database, model and chatbot\n",
    "\n",
    "To a launch a pykoi App, you only need to customize 3 components: a model, a database and a chatbot. Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_database = QuestionAnswerDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pykoi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HuggingfaceModel] loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HuggingfaceModel] loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "model = ModelFactory.create_model(\n",
    "    model_source=\"huggingface\",\n",
    "    pretrained_model_name_or_path=\"tiiuae/falcon-7b\",\n",
    "    trust_remote_code=True, ## TODO: set as default\n",
    "    load_in_8bit=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = Chatbot(model=model, feedback=\"vote\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the App\n",
    "\n",
    "Now we can launch the chatbot using the model and database. Once you run the below line, you will see a Tunnel link which ends with `ngrok-free.app`. Click this link and you can see the below interface:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../image/chatbot_vote_trim_4x_crop.gif\" width=\"75%\" height=\"75%\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "You may need to set a ngrok token (one time) by uncomment the below line, add your [personal ngrok token](https://dashboard.ngrok.com/get-started/your-authtoken) and run this `ngrok config` line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ngrok config add-authtoken xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `nest_asyncio` \n",
    "Add `nest_asyncio` to avoid error such as `asyncio.run() cannot be called from a running event loop`. Since we're running another interface inside a Jupyter notebook where an asyncio event loop is already running, we'll encounter the error. (since The uvicorn.run() function uses asyncio.run(), which isn't compatible with a running event loop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [7578]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: https://a63d9b47dea54a.lhr.life\n"
     ]
    }
   ],
   "source": [
    "app = Application(debug=False, share=True)\n",
    "app.add_component(chatbot)\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize your data\n",
    "\n",
    "Once you collect enough data via the chatbot app above, you can visualize your data via the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dashboard = Dashboard(database=qa_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Application(debug=False, share=True)\n",
    "app.add_component(chatbot)\n",
    "app.add_component(qa_dashboard)\n",
    "app.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will only see one localhost link after you run the `app.run()` below. If you are on EC2 or a remote server, you can tunnel it back to your laptop via the following options:\n",
    "\n",
    "- If you are using VSCode, check [tunnel using VSCode](https://code.visualstudio.com/docs/remote/ssh#_forwarding-a-port-creating-ssh-tunnel);\n",
    "- [Directly config via EC2](https://www.opensourceforu.com/2021/09/how-to-do-reverse-tunnelling-with-the-amazon-ec2-instance/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can launch the dashboard. Click this above link and you can see the below interface:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../image/chatbot_dashboard_trim_2x.gif\" width=\"75%\" height=\"75%\" />\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
